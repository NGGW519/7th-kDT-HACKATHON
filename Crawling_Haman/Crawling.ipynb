{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07295218-f03c-4e13-a501-f4d8da8c1dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193개 항목을 sitemap_links.csv에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.haman.go.kr/02828/02829.web\"\n",
    "res = requests.get(url)\n",
    "res.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "rows = []\n",
    "columns = soup.select(\"div.sitemap1.even-grid.evenmix-1234 > div.column\")\n",
    "\n",
    "for col in columns:\n",
    "    main_title = col.select_one(\"a.d2\").get_text(strip=True)\n",
    "    for mid_li in col.select(\"ul.bu.mgt0 > li\"):\n",
    "        mid_title = mid_li.find(\"a\", recursive=False).get_text(strip=True)\n",
    "        for sub_li in mid_li.select(\"ul.bu.mgt0 > li\"):\n",
    "            a = sub_li.find(\"a\", recursive=False)\n",
    "            sub_title = a.get_text(strip=True)\n",
    "            href = a[\"href\"]\n",
    "            link = href if href.startswith(\"http\") else \"https://www.haman.go.kr\" + href\n",
    "            rows.append([main_title, mid_title, sub_title, link])\n",
    "\n",
    "with open(\"sitemap_links.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"대주제\", \"중주제\", \"소주제\", \"링크\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"{len(rows)}개 항목을 sitemap_links.csv에 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c34b6324-4055-49d8-b41e-e3202c4e2bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료: sitemap_full_contents.csv에 상세정보를 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "INPUT_CSV  = \"sitemap_links.csv\"\n",
    "OUTPUT_CSV = \"sitemap_full_contents.csv\"\n",
    "\n",
    "# 제외할 안내문 키워드 목록\n",
    "FILTER_KEYWORDS = [\n",
    "    \"함안으로 떠나는 여행맞춤정보\",\n",
    "    \"관광객의 취향을 반영한 최적의 관광지를 추천해드립니다\",\n",
    "    \"Q1\", \"Q2\", \"Q3\",\n",
    "    \"안내도 바로보기\"\n",
    "]\n",
    "\n",
    "def is_boilerplate(text):\n",
    "    return any(text.startswith(kw) for kw in FILTER_KEYWORDS)\n",
    "\n",
    "with open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as fin, \\\n",
    "     open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    reader = csv.DictReader(fin)\n",
    "    writer = csv.DictWriter(fout, fieldnames=[\"대주제\",\"중주제\",\"소주제\",\"링크\",\"상세정보\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        main, mid, sub, url = row[\"대주제\"], row[\"중주제\"], row[\"소주제\"], row[\"링크\"]\n",
    "        detail = \"\"\n",
    "        try:\n",
    "            res = requests.get(url, timeout=10)\n",
    "            res.encoding = \"utf-8\"\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "            # wrap1 우선\n",
    "            wrap = soup.select_one(\"div.info3cont > div.wrap1\") or \\\n",
    "                   soup.select_one(\"div.tour1info3.more.full\") or \\\n",
    "                   soup.select_one(\"div.infobox2\") or \\\n",
    "                   soup.select_one(\"div.tg2\")\n",
    "            if wrap:\n",
    "                # 텍스트 전체 추출, 라인별로 분리 후 불필요 항목 제외\n",
    "                texts = [line.strip() for line in wrap.get_text(separator=\"\\n\").split(\"\\n\") if line.strip()]\n",
    "                filtered = [t for t in texts if not is_boilerplate(t)]\n",
    "                detail = \"\\n\".join(filtered)\n",
    "        except Exception:\n",
    "            detail = \"\"\n",
    "\n",
    "        writer.writerow({\n",
    "            \"대주제\": main,\n",
    "            \"중주제\": mid,\n",
    "            \"소주제\": sub,\n",
    "            \"링크\": url,\n",
    "            \"상세정보\": detail\n",
    "        })\n",
    "\n",
    "print(f\"완료: {OUTPUT_CSV}에 상세정보를 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d374e616-6e76-43f1-ac2c-bbc314005d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25개 항목을 restaurant_list.csv에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.haman.go.kr/02373/02374/02379.web\"\n",
    "res = requests.get(url, timeout=10)\n",
    "res.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "rows = []\n",
    "for li in soup.select(\"div.tour1list1 ol > li.li1\"):\n",
    "    num = li.select_one(\"span.num\").get_text(strip=True)\n",
    "    name = li.select_one(\"strong.h1 a\").get_text(strip=True)\n",
    "    href = li.select_one(\"strong.h1 a\")[\"href\"]\n",
    "    detail_link = requests.compat.urljoin(url, href)\n",
    "    map_a = li.select_one(\"div.btns a.btn-link-path\")\n",
    "    address = map_a[\"data-address\"].strip() if map_a else \"\"\n",
    "    rows.append([num, name, detail_link, address])\n",
    "\n",
    "with open(\"restaurant_list.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"순번\", \"음식점명\", \"상세페이지 링크\", \"주소\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"{len(rows)}개 항목을 restaurant_list.csv에 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd6c5390-ba58-45f3-9e83-5e2d38eb7892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13개 항목을 'hanan_cafe_list.csv'에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 함안멋집(카페) 페이지 URL\n",
    "URL = \"https://www.haman.go.kr/02373/02374/03546.web\"\n",
    "OUTPUT_CSV = \"hanan_cafe_list.csv\"\n",
    "\n",
    "res = requests.get(URL, timeout=10)\n",
    "res.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "rows = []\n",
    "for li in soup.select(\"div.tour1list1 ol > li.li1\"):\n",
    "    num_tag = li.select_one(\"span.num\")\n",
    "    num = num_tag.get_text(strip=True) if num_tag else \"\"\n",
    "    \n",
    "    name_tag = li.select_one(\"strong.h1 a\")\n",
    "    name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "    href = name_tag[\"href\"] if name_tag and name_tag.has_attr(\"href\") else \"\"\n",
    "    detail_link = requests.compat.urljoin(URL, href)\n",
    "    \n",
    "    map_a = li.select_one(\"div.btns a.btn-link-path\")\n",
    "    address = map_a[\"data-address\"].strip() if map_a and map_a.has_attr(\"data-address\") else \"\"\n",
    "    lat = map_a[\"data-y\"].strip()     if map_a and map_a.has_attr(\"data-y\")     else \"\"\n",
    "    lon = map_a[\"data-x\"].strip()     if map_a and map_a.has_attr(\"data-x\")     else \"\"\n",
    "    \n",
    "    rows.append([num, name, detail_link, address, lat, lon])\n",
    "\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"순번\", \"카페명\", \"상세페이지 링크\", \"주소\", \"위도\", \"경도\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"{len(rows)}개 항목을 '{OUTPUT_CSV}'에 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3253037a-d296-4f24-a83f-63ab8fd3f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34개 항목을 hanan_matjip_detailed.csv에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.haman.go.kr/02373/02374/02380.web\"\n",
    "OUTPUT_CSV = \"hanan_matjip_detailed.csv\"\n",
    "\n",
    "res = requests.get(URL, timeout=10)\n",
    "res.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "rows = []\n",
    "for li in soup.select(\"div.tour1list1 ol > li.li1\"):\n",
    "    num_tag = li.select_one(\"span.num\")\n",
    "    num = num_tag.get_text(strip=True) if num_tag else \"\"\n",
    "    \n",
    "    name_tag = li.select_one(\"strong.h1 a\")\n",
    "    name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "    href = name_tag[\"href\"] if name_tag and name_tag.has_attr(\"href\") else \"\"\n",
    "    detail_link = requests.compat.urljoin(URL, href)\n",
    "    \n",
    "    map_a = li.select_one(\"div.btns a.btn-link-path\")\n",
    "    address = map_a[\"data-address\"].strip() if map_a and map_a.has_attr(\"data-address\") else \"\"\n",
    "    lat = map_a[\"data-y\"].strip()     if map_a and map_a.has_attr(\"data-y\")     else \"\"\n",
    "    lon = map_a[\"data-x\"].strip()     if map_a and map_a.has_attr(\"data-x\")     else \"\"\n",
    "    \n",
    "    rows.append([num, name, detail_link, address, lat, lon])\n",
    "\n",
    "with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"순번\", \"음식점명\", \"상세페이지 링크\", \"주소\", \"위도\", \"경도\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"{len(rows)}개 항목을 {OUTPUT_CSV}에 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e9fe5b5-ed66-4655-8505-fa93e2810076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료: ansim_sikdang.csv 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.haman.go.kr/02373/02374/02381.web\"\n",
    "res = requests.get(url, timeout=10)\n",
    "res.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "with open(\"ansim_sikdang.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"연번\", \"업종\", \"업태\", \"업소명\", \"소재지\", \"전화번호\"])\n",
    "\n",
    "    for tr in soup.select(\"table.t3.tttac.ttvam tbody.tdtac tr\"):\n",
    "        cells = tr.find_all([\"th\", \"td\"])\n",
    "        num      = cells[0].get_text(strip=True)\n",
    "        type_    = cells[1].get_text(strip=True)\n",
    "        category = cells[2].get_text(strip=True)\n",
    "        name     = cells[3].get_text(strip=True)\n",
    "        address  = cells[4].get_text(strip=True)\n",
    "\n",
    "        # 전화번호 셀 추출 and 방어\n",
    "        phone_cell = cells[5]\n",
    "        span = phone_cell.select_one(\"span.t1\")\n",
    "        if span:\n",
    "            phone = span.get_text(strip=True)\n",
    "        else:\n",
    "            # <a> 자체에 텍스트가 있으면 그걸 쓰거나, td 전체 텍스트 사용\n",
    "            phone = phone_cell.get_text(strip=True)\n",
    "\n",
    "        writer.writerow([num, type_, category, name, address, phone])\n",
    "\n",
    "\n",
    "print(\"완료: ansim_sikdang.csv 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "295da8a5-8dcd-43f0-a800-2d0233c82ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legends_links.csv에 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Legend list pages\n",
    "pages = [\n",
    "    \"https://www.haman.go.kr/01834/01862/02166.web?cpage=1\",\n",
    "    \"https://www.haman.go.kr/01834/01862/02166.web?cpage=2\",\n",
    "    \"https://www.haman.go.kr/01834/01862/02166.web?cpage=3\"\n",
    "]\n",
    "\n",
    "output_file = \"legends_links.csv\"\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"제목\", \"링크\"])\n",
    "    \n",
    "    for page in pages:\n",
    "        res = requests.get(page, timeout=10)\n",
    "        res.encoding = \"utf-8\"\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        \n",
    "        # 각 전설 항목의 <a> 태그 선택\n",
    "        for a in soup.select(\"li > a[href*='?amode=view']\"):\n",
    "            title_tag = a.select_one(\"strong.h1\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "            href = a[\"href\"]\n",
    "            # 절대 URL 생성\n",
    "            link = href if href.startswith(\"http\") else requests.compat.urljoin(page, href)\n",
    "            writer.writerow([title, link])\n",
    "\n",
    "print(f\"{output_file}에 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "265d8458-ce4f-4223-a6f9-b27aab8c922b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료: legends_with_details.csv에 상세정보 열을 추가하여 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "INPUT_CSV = \"legends_links.csv\"\n",
    "OUTPUT_CSV = \"legends_with_details.csv\"\n",
    "\n",
    "with open(INPUT_CSV, newline=\"\", encoding=\"utf-8\") as fin, \\\n",
    "     open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "    reader = csv.DictReader(fin)\n",
    "    fieldnames = reader.fieldnames + [\"상세정보\"]\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        url = row[\"링크\"]\n",
    "        detail_texts = []\n",
    "        try:\n",
    "            res = requests.get(url, timeout=10)\n",
    "            res.encoding = \"utf-8\"\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            substance = soup.select_one(\"div.wrap1 div.substance\")\n",
    "            if substance:\n",
    "                # 줄바꿈 구분으로 모든 텍스트 추출\n",
    "                lines = [line.strip() for line in substance.get_text(separator=\"\\n\").split(\"\\n\") if line.strip()]\n",
    "                detail_texts = lines\n",
    "        except Exception:\n",
    "            detail_texts = []\n",
    "\n",
    "        row[\"상세정보\"] = \"\\n\".join(detail_texts)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"완료: {OUTPUT_CSV}에 상세정보 열을 추가하여 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "192d7d15-0218-47d7-93d9-2676d191adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료: persons.csv에 인물별 이름·링크·특칭·설명을 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1) 인물 페이지 URL 리스트\n",
    "base = \"https://www.haman.go.kr/01834/01862/\"\n",
    "indices = list(range(2186, 2210))  # 02186 ~ 02209\n",
    "urls = [f\"{base}{idx:05d}.web?amode=view&idx={idx}\" for idx in indices]\n",
    "\n",
    "# 2) CSV 파일 생성\n",
    "with open(\"persons.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"이름\", \"링크\", \"특칭\", \"설명\"])\n",
    "\n",
    "    for url in urls:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        res.encoding = \"utf-8\"\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        \n",
    "        # 이름: 파일명에서 추출하거나 <strong.h1> 대신 목차에서 미리 알고 있는 경우 대체\n",
    "        # 여기서는 특칭에서 괄호 앞까지를 이름으로 취급\n",
    "        h2 = soup.select_one(\"h2.h1\")\n",
    "        special = h2.get_text(\" \", strip=True) if h2 else \"\"\n",
    "        # 예: \"홍건적을 물리친, 이방실 장군 (1298~1362)\"\n",
    "        # 이름은 마지막 쉼표 이후 첫 어절로 추출\n",
    "        name = special.split(\",\")[-1].split()[0] if \",\" in special else special.split()[0]\n",
    "        \n",
    "        # 설명: wrap1 > substance 내 <p>\n",
    "        desc_block = soup.select_one(\"div.wrap1 div.substance\")\n",
    "        if desc_block:\n",
    "            ps = [p.get_text(\" \", strip=True) for p in desc_block.find_all(\"p\") if p.get_text(strip=True)]\n",
    "            description = \"\\n\".join(ps)\n",
    "        else:\n",
    "            description = \"\"\n",
    "        \n",
    "        writer.writerow([name, url, special, description])\n",
    "\n",
    "print(\"완료: persons.csv에 인물별 이름·링크·특칭·설명을 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b785d562-2ab2-496c-8132-6a28e339e3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료: '상세정보'가 비어 있지 않은 130개 행을 sitemap_full_contents_filtered.csv에 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "INPUT_CSV = \"sitemap_full_contents.csv\"\n",
    "OUTPUT_CSV = \"sitemap_full_contents_filtered.csv\"\n",
    "\n",
    "# CSV 불러오기\n",
    "df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
    "\n",
    "# '상세정보' 열이 비어 있거나 NaN인 행 제거\n",
    "df_filtered = df[df[\"상세정보\"].notna() & (df[\"상세정보\"].str.strip() != \"\")]\n",
    "\n",
    "# 결과를 새 파일로 저장\n",
    "df_filtered.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"완료: '상세정보'가 비어 있지 않은 {len(df_filtered)}개 행을 {OUTPUT_CSV}에 저장했습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
