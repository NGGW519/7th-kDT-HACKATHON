from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_tools_agent
import json

from .AgentState import AgentState
from . import AgentSystemPrompt
from ..core.config import settings
from .tools.post_tools import create_post_tool

# --- Agent Setup --- #
def create_generate_post_agent():
    """Generate Post Agent ìƒì„±"""
    
    # System prompt ì„¤ì •
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an expert copywriter. The user wants you to write a community post.

IMPORTANT: Look at the chat history carefully. If there's already a detailed post format or content suggestion from a previous conversation, USE THAT as the basis for your post. Don't create completely different content.

You have access to the following tool:
create_post_tool(title: str, content: str, category: str, token: str) -> dict
create_post_tool: ê²Œì‹œê¸€ì„ ìƒì„±í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

To create a post, you MUST use the 'create_post_tool' tool. Your response MUST be a tool call. Do NOT generate the post content directly.

The category must be one of: "ì¼ìƒ", "ë§›ì§‘", "ì¶”ì–µ", "ê¸°íƒ€".

Respond in Korean.

Guidelines:
1. Check the chat history first for any previous post suggestions or formats
2. If there's a detailed post content from previous messages, use that as the basis
3. Extract the key information (title, content, appropriate category) from the conversation context
4. Only if no previous content exists, create new appropriate content

Always use the token that will be provided in the input when calling create_post_tool."""),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}")
    ])
    
    # LLM ì„¤ì •
    llm = ChatOpenAI(
        model="gpt-4o",
        api_key=settings.OPENAI_API_KEY,
        temperature=0.7,
        streaming=True
    )
    
    # Tools ì„¤ì •
    tools = [create_post_tool]
    
    # Agent ìƒì„±
    agent = create_openai_tools_agent(llm, tools, prompt)
    
    # AgentExecutor ìƒì„±
    agent_executor = AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True,
        handle_parsing_errors=True,
        return_intermediate_steps=True
    )
    
    return agent_executor

# --- Main Agent Function --- #
async def run_generate_post_agent(state: AgentState):
    """Generates a post using AgentExecutor with streaming."""
    print("--- Running Generate Post Agent with Tool Calling ---")
    
    user_id = state.get("user_id")
    token = state.get("token")
    prompt = state.get("prompt")
    
    if not user_id or not token:
        error_msg = "ì‚¬ìš©ì ì¸ì¦ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        print(f"Error: {error_msg}")
        yield {"generation": error_msg}
        return
    
    if not prompt:
        error_msg = "ìƒì„±í•  ê²Œì‹œê¸€ì— ëŒ€í•œ ìš”ì²­ì´ ì—†ìŠµë‹ˆë‹¤."
        print(f"Error: {error_msg}")
        yield {"generation": error_msg}
        return
    
    try:
        # Agent ìƒì„±
        agent_executor = create_generate_post_agent()
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        messages = state.get("messages", [])
        chat_history = []
        for msg in messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸ (í˜„ì¬ ìš”ì²­)
            chat_history.append(msg)
        
        # Agent ì‹¤í–‰ì„ ìœ„í•œ ì…ë ¥ ì¤€ë¹„
        agent_input = {
            "input": f"""ì‚¬ìš©ì ìš”ì²­: {prompt}

ì¸ì¦ í† í°: {token}

ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²Œì‹œê¸€ì„ create_post_toolì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•˜ê³  ì €ì¥í•´ì£¼ì„¸ìš”.
ë§Œì•½ ì´ì „ ëŒ€í™”ì—ì„œ ê²Œì‹œê¸€ ë‚´ìš©ì´ë‚˜ í¬ë§·ì´ ì œì•ˆë˜ì—ˆë‹¤ë©´ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì„¸ìš”.
í† í° ë§¤ê°œë³€ìˆ˜ì—ëŠ” ìœ„ì— ì œê³µëœ í† í°ì„ ì‚¬ìš©í•˜ì„¸ìš”.""",
            "chat_history": chat_history
        }
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        full_response = ""
        async for chunk in agent_executor.astream(agent_input):
            # ì¤‘ê°„ ë‹¨ê³„ë‚˜ ìµœì¢… ê²°ê³¼ ì²˜ë¦¬
            if "output" in chunk:
                content = chunk["output"]
                full_response += content
                yield content
            elif "intermediate_steps" in chunk:
                # ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                for step in chunk["intermediate_steps"]:
                    if len(step) >= 2:
                        tool_result = step[1]  # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼
                        if isinstance(tool_result, dict) and tool_result.get("status") == "success":
                            success_msg = f"\nâœ… {tool_result.get('message', 'ê²Œì‹œê¸€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.')}"
                            yield success_msg
                            full_response += success_msg
        
        # ìµœì¢… ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
        if not full_response.strip():
            final_msg = "ê²Œì‹œê¸€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
            yield final_msg
            full_response = final_msg
            
        # Update task index for work plan progression
        current_index = state.get("current_task_index", 0)
        yield {"generation": full_response, "current_task_index": current_index + 1}
        
    except Exception as e:
        error_msg = f"ê²Œì‹œê¸€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        print(f"Agent execution error: {e}")
        yield {"generation": error_msg}

# --- Alternative: Simpler Direct Approach --- #
async def run_generate_post_agent_simple(state: AgentState):
    """ë” ê°„ë‹¨í•œ ë²„ì „: LLMìœ¼ë¡œ ë‚´ìš© ìƒì„± í›„ ë°”ë¡œ ë„êµ¬ í˜¸ì¶œ"""
    print("--- Running Generate Post Agent (Simple Version) ---")
    
    user_id = state.get("user_id")
    token = state.get("token")
    prompt = state.get("prompt")
    
    if not all([user_id, token, prompt]):
        error_msg = "í•„ìˆ˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        yield {"generation": error_msg}
        return
    
    try:
        # 1ë‹¨ê³„: LLMìœ¼ë¡œ ê²Œì‹œê¸€ ë‚´ìš© ìƒì„±
        generation_prompt = ChatPromptTemplate.from_messages([
            ("system", f"""{AgentSystemPrompt.GENERATE_POST_PROMPT}
            
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ ê²Œì‹œê¸€ì„ ìƒì„±í•˜ì„¸ìš”. 
ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤:
{{
    "title": "ê²Œì‹œê¸€ ì œëª©",
    "content": "ê²Œì‹œê¸€ ë‚´ìš©", 
    "category": "ì¹´í…Œê³ ë¦¬"
}}"""),
            ("human", "{prompt}")
        ])
        
        llm = ChatOpenAI(
            model="gpt-4o", 
            api_key=settings.OPENAI_API_KEY, 
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        chain = generation_prompt | llm
        response = await chain.ainvoke({"prompt": prompt})
        
        # JSON íŒŒì‹±
        post_data = json.loads(response.content)
        title = post_data.get("title")
        content = post_data.get("content")  
        category = post_data.get("category")
        
        if not all([title, content, category]):
            yield {"generation": "ê²Œì‹œê¸€ ìƒì„± ì¤‘ ì˜¤ë¥˜: í•„ìˆ˜ í•­ëª©ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."}
            return
        
        yield f"ğŸ“ ê²Œì‹œê¸€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤:\nì œëª©: {title}\nì¹´í…Œê³ ë¦¬: {category}\n\nê²Œì‹œê¸€ì„ ì €ì¥í•˜ëŠ” ì¤‘..."
        
        # 2ë‹¨ê³„: ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥
        result = await create_post_tool(title, content, category, token)
        
        if result["status"] == "success":
            final_msg = f"âœ… ê²Œì‹œê¸€ '{title}'ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤! (ID: {result.get('post_id')})"
        else:
            final_msg = f"âŒ ê²Œì‹œê¸€ ì €ì¥ ì‹¤íŒ¨: {result.get('message')}"
            
        yield final_msg
        yield {"generation": f"ê²Œì‹œê¸€ ìƒì„± ì™„ë£Œ\nì œëª©: {title}\nìƒíƒœ: {result['status']}"}
        
    except json.JSONDecodeError:
        yield {"generation": "ê²Œì‹œê¸€ ìƒì„± ì¤‘ ì˜¤ë¥˜: AI ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    except Exception as e:
        yield {"generation": f"ê²Œì‹œê¸€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"}